{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11077089,"sourceType":"datasetVersion","datasetId":6903761}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T10:18:41.525779Z\",\"iopub.execute_input\":\"2025-03-23T10:18:41.52618Z\",\"iopub.status.idle\":\"2025-03-23T11:28:11.748434Z\",\"shell.execute_reply.started\":\"2025-03-23T10:18:41.526144Z\",\"shell.execute_reply\":\"2025-03-23T11:28:11.74748Z\"}}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\nfrom PIL import Image\nimport os\nimport re\nfrom tqdm import tqdm\n\n# ===========================\n# 1. Dataset Classes\n# ===========================\nclass PairedTrainDataset(Dataset):\n    \"\"\"\n    Training dataset with paired low-res and high-res images.\n    Expects file names to share a common numeric id.\n    \"\"\"\n    def __init__(self, lr_dir, hr_dir, lr_transform=None, hr_transform=None):\n        self.lr_dir = lr_dir\n        self.hr_dir = hr_dir\n        self.lr_files = sorted(os.listdir(lr_dir))\n        self.hr_files = sorted(os.listdir(hr_dir))\n        self.lr_transform = lr_transform\n        self.hr_transform = hr_transform\n\n        # Build mapping based on numeric parts in filenames\n        def extract_num(fname):\n            m = re.search(r'(\\d+)', fname)\n            return m.group(1).zfill(4) if m else None\n\n        self.lr_map = {extract_num(f): f for f in self.lr_files if extract_num(f)}\n        self.hr_map = {extract_num(f): f for f in self.hr_files if extract_num(f)}\n        self.common_keys = sorted(list(set(self.lr_map.keys()) & set(self.hr_map.keys())))\n        if len(self.common_keys) == 0:\n            raise ValueError(\"No matching image pairs found between LR and HR datasets.\")\n\n    def __len__(self):\n        return len(self.common_keys)\n\n    def __getitem__(self, idx):\n        key = self.common_keys[idx]\n        lr_path = os.path.join(self.lr_dir, self.lr_map[key])\n        hr_path = os.path.join(self.hr_dir, self.hr_map[key])\n        try:\n            lr_img = Image.open(lr_path).convert(\"RGB\")\n            hr_img = Image.open(hr_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading pair {lr_path} and {hr_path}: {e}\")\n            lr_img = Image.new(\"RGB\", (64, 64))\n            hr_img = Image.new(\"RGB\", (256, 256))\n        if self.lr_transform:\n            lr_img = self.lr_transform(lr_img)\n        if self.hr_transform:\n            hr_img = self.hr_transform(hr_img)\n        return lr_img, hr_img\n\nclass SingleImageDataset(Dataset):\n    \"\"\"\n    For validation and testing: uses only low-res images.\n    \"\"\"\n    def __init__(self, lr_dir, transform=None):\n        self.lr_dir = lr_dir\n        self.lr_files = sorted(os.listdir(lr_dir)) if os.path.exists(lr_dir) else []\n        self.transform = transform\n        if not self.lr_files:\n            print(f\"Warning: No images found in {lr_dir}\")\n\n    def __len__(self):\n        return len(self.lr_files)\n\n    def __getitem__(self, idx):\n        fname = self.lr_files[idx]\n        lr_path = os.path.join(self.lr_dir, fname)\n        try:\n            img = Image.open(lr_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {lr_path}: {e}\")\n            img = Image.new(\"RGB\", (64, 64))\n        if self.transform:\n            img = self.transform(img)\n        return img, fname\n\n# ===========================\n# 2. Image Transformations\n# ===========================\n# Training transforms (paired LR/HR)\ntrain_lr_transform = T.Compose([\n    T.Resize((64, 64)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\ntrain_hr_transform = T.Compose([\n    T.Resize((256, 256)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n# For validation and test (LR images)\nval_lr_transform = T.Compose([\n    T.Resize((64, 64)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n\n# ===========================\n# 3. RCAN Model Definition\n# ===========================\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv(y)\n        return x * y\n\nclass RCAB(nn.Module):\n    \"\"\"Residual Channel Attention Block\"\"\"\n    def __init__(self, channels, reduction=16):\n        super(RCAB, self).__init__()\n        self.body = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        )\n        self.ca = ChannelAttention(channels, reduction)\n\n    def forward(self, x):\n        out = self.body(x)\n        out = self.ca(out)\n        return x + out\n\nclass ResidualGroup(nn.Module):\n    def __init__(self, channels, n_RCAB, reduction=16):\n        super(ResidualGroup, self).__init__()\n        modules = [RCAB(channels, reduction) for _ in range(n_RCAB)]\n        modules.append(nn.Conv2d(channels, channels, kernel_size=3, padding=1))\n        self.body = nn.Sequential(*modules)\n\n    def forward(self, x):\n        res = self.body(x)\n        return x + res\n\nclass RCAN(nn.Module):\n    def __init__(self, scale_factor=4, n_resgroups=10, n_RCAB=20, channels=64, reduction=16):\n        super(RCAN, self).__init__()\n        # Shallow feature extraction\n        self.conv1 = nn.Conv2d(3, channels, kernel_size=3, padding=1)\n        # Residual groups\n        self.resgroups = nn.Sequential(*[ResidualGroup(channels, n_RCAB, reduction) for _ in range(n_resgroups)])\n        # Conv after groups\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        # Upscaling using PixelShuffle (assuming scale_factor is power of 2)\n        upscaling = []\n        for _ in range(int(scale_factor//2)):\n            upscaling += [\n                nn.Conv2d(channels, channels * 4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n                nn.ReLU(inplace=True)\n            ]\n        self.upscale = nn.Sequential(*upscaling)\n        # Reconstruction\n        self.conv3 = nn.Conv2d(channels, 3, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        res = self.resgroups(x)\n        x = self.conv2(res) + x\n        x = self.upscale(x)\n        x = self.conv3(x)\n        return x\n\n# ===========================\n# 4. Training Function\n# ===========================\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    epoch_loss = 0\n    count = 0\n    for lr, hr in tqdm(dataloader, desc=\"Training\"):\n        lr, hr = lr.to(device), hr.to(device)\n        optimizer.zero_grad()\n        sr = model(lr)\n        loss = criterion(sr, hr)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        count += 1\n    return epoch_loss / count if count > 0 else float('inf')\n\n# ===========================\n# 5. Output Generation Function\n# ===========================\ndef generate_outputs(model, dataloader, device, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    # Inverse normalization: convert from [-1,1] to [0,1]\n    inv_normalize = T.Compose([\n        T.Normalize(mean=[0.0]*3, std=[1/0.5]*3),\n        T.Normalize(mean=[-0.5]*3, std=[1.0]*3)\n    ])\n    model.eval()\n    with torch.no_grad():\n        for lr, fname in tqdm(dataloader, desc=\"Generating Outputs\"):\n            lr = lr.to(device)\n            sr = model(lr).clamp(-1, 1)\n            sr = inv_normalize(sr.squeeze(0).cpu())\n            sr_img = T.ToPILImage()(sr)\n            sr_img.save(os.path.join(output_dir, fname[0]))\n\n# ===========================\n# 6. Main Function\n# ===========================\ndef main(\n    train_lr_dir, train_hr_dir, \n    val_lr_dir,\n    test_lr_dir,\n    output_dir='./rcan_results',\n    batch_size=4,\n    num_epochs=50,\n    scale_factor=4\n):\n    print(\"Starting training with RCAN...\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create the training dataset (paired low-res and high-res)\n    train_dataset = PairedTrainDataset(train_lr_dir, train_hr_dir, train_lr_transform, train_hr_transform)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    \n    # Create validation dataset (only low-res images)\n    val_dataset = SingleImageDataset(val_lr_dir, transform=val_lr_transform)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n    \n    # Create test dataset (only low-res images)\n    test_dataset = SingleImageDataset(test_lr_dir, transform=val_lr_transform)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n    \n    # Initialize RCAN model\n    model = RCAN(scale_factor=scale_factor, n_resgroups=10, n_RCAB=20, channels=64, reduction=16).to(device)\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"RCAN model created with {total_params:,} parameters.\")\n    \n    criterion = nn.L1Loss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    \n    best_loss = float('inf')\n    os.makedirs(output_dir, exist_ok=True)\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n        \n        # Save best model based on training loss\n        if train_loss < best_loss:\n            best_loss = train_loss\n            torch.save(model.state_dict(), os.path.join(output_dir, 'rcan_best.pth'))\n            print(f\"Saved best model (Train Loss = {best_loss:.4f})\")\n        \n        # Save periodic checkpoint\n        torch.save({\n            'epoch': epoch+1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'train_loss': train_loss\n        }, os.path.join(output_dir, f'rcan_checkpoint_epoch_{epoch+1}.pth'))\n    \n    # Load best model for generating outputs\n    best_model_path = os.path.join(output_dir, 'rcan_best.pth')\n    if os.path.exists(best_model_path):\n        model.load_state_dict(torch.load(best_model_path, map_location=device))\n        print(\"Loaded best model for output generation.\")\n    \n    # Generate outputs on validation set for visual inspection\n    val_output_dir = os.path.join(output_dir, 'validation_results')\n    generate_outputs(model, val_loader, device, val_output_dir)\n    print(\"Validation outputs generated successfully!\")\n    \n    # Generate outputs on test set\n    test_output_dir = os.path.join(output_dir, 'test_results')\n    generate_outputs(model, test_loader, device, test_output_dir)\n    print(\"Test outputs generated successfully!\")\n\nif __name__ == \"__main__\":\n    # Define your directories accordingly\n    train_lr_dir = \"/kaggle/input/resolution-x4-photos/DIV2K_train_LR_bicubic/DIV2K_train_LR_bicubic/X4\"\n    train_hr_dir = \"/kaggle/input/resolution-x4-photos/DIV2K_train_HR/DIV2K_train_HR\"\n    val_lr_dir   = \"/kaggle/input/resolution-x4-photos/DIV2K_valid_LR_bicubic/DIV2K_valid_LR_bicubic/X4\"   # Only low-res images available for validation\n    test_lr_dir  = \"/kaggle/input/resolution-x4-photos/DIV2K_test_LR_bicubic/DIV2K_test_LR_bicubic/X4\"\n    \n    main(\n        train_lr_dir=train_lr_dir,\n        train_hr_dir=train_hr_dir,\n        val_lr_dir=val_lr_dir,\n        test_lr_dir=test_lr_dir,\n        output_dir='./rcan_results',\n        batch_size=4,\n        num_epochs=50,\n        scale_factor=4\n    )\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T11:31:15.644388Z\",\"iopub.execute_input\":\"2025-03-23T11:31:15.644764Z\",\"iopub.status.idle\":\"2025-03-23T11:31:16.160757Z\",\"shell.execute_reply.started\":\"2025-03-23T11:31:15.64473Z\",\"shell.execute_reply\":\"2025-03-23T11:31:16.159903Z\"}}\n!zip -r test_results.zip /kaggle/working/rcan_results/test_results\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T11:31:45.371467Z\",\"iopub.execute_input\":\"2025-03-23T11:31:45.371819Z\",\"iopub.status.idle\":\"2025-03-23T11:31:45.376851Z\",\"shell.execute_reply.started\":\"2025-03-23T11:31:45.371792Z\",\"shell.execute_reply\":\"2025-03-23T11:31:45.376187Z\"}}\nfrom IPython.display import FileLink\n\nFileLink(\"/kaggle/working/test_results.zip\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T11:50:07.452303Z\",\"iopub.execute_input\":\"2025-03-23T11:50:07.452627Z\",\"iopub.status.idle\":\"2025-03-23T11:50:08.295073Z\",\"shell.execute_reply.started\":\"2025-03-23T11:50:07.452603Z\",\"shell.execute_reply\":\"2025-03-23T11:50:08.294299Z\"}}\nimport os\nimport cv2\n\ninput_folder = '/kaggle/working/rcan_results/test_results'\noutput_folder = '/kaggle/working/final_submission'\n\nos.makedirs(output_folder, exist_ok=True)\n\nfor filename in os.listdir(input_folder):\n    if filename.endswith(\".png\"):\n        img_path = os.path.join(input_folder, filename)\n        img = cv2.imread(img_path)\n\n        # Save with lossless PNG compression\n        cv2.imwrite(os.path.join(output_folder, filename), img, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n\nprint(\"✅ All images saved with lossless compression!\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T11:50:55.314851Z\",\"iopub.execute_input\":\"2025-03-23T11:50:55.315149Z\",\"iopub.status.idle\":\"2025-03-23T11:50:55.32017Z\",\"shell.execute_reply.started\":\"2025-03-23T11:50:55.315127Z\",\"shell.execute_reply\":\"2025-03-23T11:50:55.319455Z\"}}\nreadme_content = \"\"\"runtime per image [s] : 1.00\nCPU[1] / GPU[0] : 0\nExtra Data [1] / No Extra Data [0] : 0\nOther description : Solution based on RCAN (Residual Channel Attention Network). Implemented using PyTorch and tested on Kaggle's GPU. No extra data was used for training.\n\"\"\"\n\nwith open('/kaggle/working/final_submission/readme.txt', 'w') as f:\n    f.write(readme_content)\n\nprint(\"✅ readme.txt created successfully!\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T11:51:13.948227Z\",\"iopub.execute_input\":\"2025-03-23T11:51:13.948564Z\",\"iopub.status.idle\":\"2025-03-23T11:51:16.391985Z\",\"shell.execute_reply.started\":\"2025-03-23T11:51:13.948534Z\",\"shell.execute_reply\":\"2025-03-23T11:51:16.390778Z\"}}\n!cd /kaggle/working/final_submission && zip -r submission.zip .\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T11:59:36.417107Z\",\"iopub.execute_input\":\"2025-03-23T11:59:36.417432Z\",\"iopub.status.idle\":\"2025-03-23T11:59:36.422996Z\",\"shell.execute_reply.started\":\"2025-03-23T11:59:36.417408Z\",\"shell.execute_reply\":\"2025-03-23T11:59:36.422275Z\"}}\nimport os\n\n# Define the new submission folder\nnew_submission_folder = '/kaggle/working/final_submission_v1'\nos.makedirs(new_submission_folder, exist_ok=True)\n\nprint(f\"📁 New submission folder created: {new_submission_folder}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T12:00:13.010383Z\",\"iopub.execute_input\":\"2025-03-23T12:00:13.010739Z\",\"iopub.status.idle\":\"2025-03-23T12:00:24.568625Z\",\"shell.execute_reply.started\":\"2025-03-23T12:00:13.010712Z\",\"shell.execute_reply\":\"2025-03-23T12:00:24.567738Z\"}}\nimport cv2\nimport shutil\n\n# Paths\ninput_folder = '/kaggle/input/resolution-x4-photos/DIV2K_test_LR_bicubic/DIV2K_test_LR_bicubic/X4'  # Low-resolution inputs\noutput_folder = '/kaggle/working/rcan_results/test_results'  # Existing SR outputs\n\nscale_factor = 4\n\n# Process images and fix mismatches\nfor filename in os.listdir(input_folder):\n    if filename.endswith('.png'):\n        lr_image_path = os.path.join(input_folder, filename)\n        sr_image_path = os.path.join(output_folder, filename)\n\n        # Load images\n        lr_image = cv2.imread(lr_image_path)\n        sr_image = cv2.imread(sr_image_path)\n\n        if sr_image is None:\n            print(f\"❌ Missing SR image for: {filename}\")\n            continue\n\n        # Expected SR dimensions\n        expected_size = (lr_image.shape[1] * scale_factor, lr_image.shape[0] * scale_factor)\n\n        # If size mismatch, resize the SR image\n        if sr_image.shape[:2] != expected_size[::-1]:\n            print(f\"🔧 Fixing size mismatch for: {filename}\")\n            sr_image = cv2.resize(sr_image, expected_size, interpolation=cv2.INTER_CUBIC)\n\n        # Save the (fixed) image to the new submission folder\n        output_path = os.path.join(new_submission_folder, filename)\n        cv2.imwrite(output_path, sr_image, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n\nprint(\"✅ Image verification and correction complete!\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T12:00:38.321137Z\",\"iopub.execute_input\":\"2025-03-23T12:00:38.32145Z\",\"iopub.status.idle\":\"2025-03-23T12:00:38.326659Z\",\"shell.execute_reply.started\":\"2025-03-23T12:00:38.321421Z\",\"shell.execute_reply\":\"2025-03-23T12:00:38.325857Z\"}}\nreadme_content = \"\"\"runtime per image [s] : 1.00\nCPU[1] / GPU[0] : 0\nExtra Data [1] / No Extra Data [0] : 0\nOther description : Solution based on RCAN (Residual Channel Attention Network). Implemented using PyTorch and tested on Kaggle's GPU. No extra data was used for training.\n\"\"\"\n\nwith open(f'{new_submission_folder}/readme.txt', 'w') as f:\n    f.write(readme_content)\n\nprint(\"✅ readme.txt created successfully!\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-23T12:00:56.107719Z\",\"iopub.execute_input\":\"2025-03-23T12:00:56.108012Z\",\"iopub.status.idle\":\"2025-03-23T12:02:39.722417Z\",\"shell.execute_reply.started\":\"2025-03-23T12:00:56.10799Z\",\"shell.execute_reply\":\"2025-03-23T12:02:39.721281Z\"}}\n!cd /kaggle/working/final_submission_v1 && zip -r ../final_submission_v1.zip .\n","metadata":{"_uuid":"d8d4ccb3-3026-4b4e-b4af-709437b004b8","_cell_guid":"106aa0e8-9aa8-44fc-b0e9-eb082bed08a1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}